{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets\n",
    "We begin by loading both the Boston and Iris datasets. More information about the Boston dataset can be found [here](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and more information about the Iris dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/iris). Do not change the cell below this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a random state for reproducibility.\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "# Load and shuffle the Boston dataset. Only subsample some features.\n",
    "boston_X, boston_y = load_boston(return_X_y=True)\n",
    "permutation = random_state.permutation(boston_X.shape[0])\n",
    "boston_X = boston_X[permutation][:, [5, 6]]\n",
    "boston_y = boston_y[permutation]\n",
    "\n",
    "# Split the dataset into train and test sets.\n",
    "boston_X_train = boston_X[:-100]\n",
    "boston_y_train = boston_y[:-100]\n",
    "boston_X_test = boston_X[-100:]\n",
    "boston_y_test = boston_y[-100:]\n",
    "\n",
    "# Create a new featurization for the boston dataset by turning the current\n",
    "# features into a tenth degree polynomial.\n",
    "boston_poly_X_train = PolynomialFeatures(8).fit_transform(boston_X_train)\n",
    "boston_poly_X_test = PolynomialFeatures(8).fit_transform(boston_X_test)\n",
    "\n",
    "# Now load and shuffle the Iris dataset.\n",
    "# Discarding all output labels that correspond to a 2.\n",
    "iris_X, iris_y = load_iris(return_X_y=True)\n",
    "iris_X = iris_X[:100]\n",
    "iris_y = iris_y[:100]\n",
    "permutation = random_state.permutation(iris_X.shape[0])\n",
    "iris_X = iris_X[permutation]\n",
    "iris_y = iris_y[permutation]\n",
    "\n",
    "# Split the dataset into train and test sets.\n",
    "iris_X_train = iris_X[:-20]\n",
    "iris_y_train = iris_y[:-20]\n",
    "iris_X_test = iris_X[-20:]\n",
    "iris_y_test = iris_y[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Prediction functions\n",
    "We now define the regression and prediction functions. You need to fill these out, we provide `logistic_cross_entropy_loss_gradient` as an example.\n",
    "\n",
    "Remember that the squared loss (with regularization) with respect to linear regression is defined as $\\|X\\beta - y\\|_2^2 + \\lambda \\|\\beta\\|_2^2$ where $\\beta$ is the linear model, $X$ is the feature matrix, $\\lambda$ is a regularization term, and $y$ are the true output values. Furthermore remember that the derivative of $\\|X\\beta - y\\|_2^2$ with respect to $\\beta$ is $2X^{\\top}(X\\beta - y)$ and the derivative of $\\lambda \\|\\beta\\|_2^2$ is $2\\lambda \\beta$.\n",
    "\n",
    "The cross entropy loss with respect to logistic regression is defined as $-\\frac{1}{n} \\sum_{i=1}^n \\left[y_i \\log \\sigma(x_i^{\\top}\\beta) + (1 - y_i)\\log\\sigma(-x_i^{\\top}\\beta)\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_predict(X, beta):\n",
    "    \"\"\"Given a linear model (aka a vector) and a feature matrix\n",
    "    predict the output vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array of shape n\n",
    "        The predicted output vector.\n",
    "    \"\"\"\n",
    "    # TODO: Fill in (Q2b)\n",
    "    return X @ beta\n",
    "\n",
    "def regression_least_squares(X, true_y, lambda_value):\n",
    "    \"\"\"Compute the optimal linear model that minimizes the regularized squared loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vector.\n",
    "    lambda_value : float\n",
    "        A non-negative regularization term.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    beta : numpy array of shape d\n",
    "        The optimal linear model.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    inv_term = np.linalg.inv((X.T @ X)/n + lambda_value * np.identity(d))\n",
    "    next_term =  X.T @ true_y /n\n",
    "    return inv_term @ next_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01444146, -0.13028296,  0.23174124,  0.20650694])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_least_squares(iris_X_train, iris_y_train, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_predict(X, beta):\n",
    "    \"\"\"Given a linear model (aka a vector) and a feature matrix\n",
    "    predict the probability of the output label being 1 using logistic\n",
    "    regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array of shape n\n",
    "        The predicted output vector.\n",
    "    \"\"\"\n",
    "    # TODO: Fill in (Q2b)\n",
    "    return sigmoid(X @ beta)\n",
    "    \n",
    "\n",
    "def logistic_cross_entropy_loss(X, beta, true_y):\n",
    "    \"\"\"Output the cross entropy loss of a given logistic model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors. Consists of 0s and 1s.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        The value of the loss.\n",
    "    \"\"\"\n",
    "    return -1/len(true_y) * np.sum(true_y*np.log(sigmoid(X @ beta)) + (1-true_y) * np.log(sigmoid(-X@beta)))\n",
    "\n",
    "def logistic_cross_entropy_loss_gradient(X, beta, true_y):\n",
    "    \"\"\"Output the gradient of the squared loss evaluated with respect to beta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss_gradient : numpy array of shape d\n",
    "        The gradient of the loss evaluated with respect to beta.\n",
    "    \"\"\"\n",
    "    return -np.sum((true_y - sigmoid(X @ beta)) * X.T, axis=1) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52107428943243517"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = iris_X_train\n",
    "true_y = iris_y_train\n",
    "logistic_cross_entropy_loss(x, regression_least_squares(x, true_y, 0.01), true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, init_beta, true_y, loss, loss_gradient,\n",
    "                     learning_rate, iterations):\n",
    "    \"\"\"Performs gradient descent on a given loss function and\n",
    "    returns the optimized beta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    init_beta : numpy array of shape d\n",
    "        The initial value for the linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors.\n",
    "    loss : function\n",
    "        The loss function we are optimizing.\n",
    "    loss_gradient : function\n",
    "        The gradient function that corresponds to the loss function.\n",
    "    learning_rate : float\n",
    "        The learning rate for gradient descent.\n",
    "    iterations : int\n",
    "        The number of iterations to optimize the loss for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : numpy array of shape d\n",
    "        The optimized beta.\n",
    "    \"\"\"\n",
    "    beta = init_beta\n",
    "    for i in range(iterations):\n",
    "        gradient = loss_gradient(X, beta, true_y)\n",
    "        new_beta = beta - learning_rate * gradient\n",
    "        if abs(new_beta[0] - beta[0]) < 0.001:\n",
    "            print(\"iteration\", i)\n",
    "            return new_beta\n",
    "        beta = new_beta\n",
    "    return new_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.41649302, -1.46333194,  2.14453759,  1.04725936])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testing gradient_descent\n",
    "\n",
    "X = iris_X_train\n",
    "true_y = iris_y_train\n",
    "init_beta = regression_least_squares(X, true_y, 0.01)\n",
    "learning_rate = 0.2\n",
    "iterations = 1000\n",
    "\n",
    "gradient_descent(X, init_beta, true_y, logistic_cross_entropy_loss, logistic_cross_entropy_loss_gradient, learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models for the Boston dataset\n",
    "In the section below you will train a regression model and evaluate it against the RMSE for the Boston housing dataset we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## RMSE function\n",
    "\n",
    "def RMSE(preds, true_y):\n",
    "    RMSE = np.sqrt((1/len(true_y) * np.sum((true_y - preds)**2))) \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.6563463275551848,\n",
       " 6.6563817002689527,\n",
       " 6.6567002481403614,\n",
       " 6.6599048020484881,\n",
       " 6.6937834263964371,\n",
       " 7.1557274608771735,\n",
       " 11.098618724445203]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# RMSE for Boston Housing Datase\n",
    "x_train = boston_X_train\n",
    "x_test= boston_X_test\n",
    "\n",
    "lambdas = [0, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "rmse_vals = []\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    beta = regression_least_squares(x_train,boston_y_train,lambdas[i])\n",
    "    pred = regression_predict(x_test, beta)\n",
    "    rmse_vals += [RMSE(pred, boston_y_test)]\n",
    "    \n",
    "rmse_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121b34fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = pd.DataFrame(data={'lambdas': lambdas, 'rmse': rmse_vals})\n",
    "sns.pointplot(x='lambdas', y='rmse', data=d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16.48584680844737,\n",
       " 5.4658120765808373,\n",
       " 5.082927772023984,\n",
       " 4.965830778537021,\n",
       " 4.9103477010684395,\n",
       " 4.9201698739985309,\n",
       " 4.9204280221606274]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = [0, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "rmse_vals = []\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    beta = regression_least_squares(boston_poly_X_train, boston_y_train,lambdas[i])\n",
    "    pred = regression_predict(boston_poly_X_test, beta)\n",
    "    rmse_vals += [RMSE(pred, boston_y_test)]\n",
    "    \n",
    "rmse_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAeaMQmLTXWpNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121c52e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = pd.DataFrame(data={'lambdas': lambdas, 'rmse': rmse_vals})\n",
    "sns.pointplot(x='lambdas', y='rmse', data=d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2d: Written response\n",
    "\n",
    "The value of regularization term did matter for both featurizations, in different ways.\n",
    "The second featurization performed better because the RMSE was lower overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model for the logistic regression dataset\n",
    "In this section you will train a logistic model and evaluate it against the MAE for the Iris dataset we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Fill in (Q2f)\n",
    "def MAE(preds, true_y):\n",
    "    mae = np.sum(np.absolute((true_y - preds)))\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Traiing logistic regression\n",
    "# Use gradient descent to find betas\n",
    "# Use betas in logistic predict\n",
    "\n",
    "MAE_vals = []\n",
    "init_beta = regression_least_squares(iris_X_train, iris_y_train, 10) ### CHECK\n",
    "\n",
    "betas = gradient_descent(iris_X_train, init_beta, iris_y_train, logistic_cross_entropy_loss, \n",
    "                        logistic_cross_entropy_loss_gradient, 0.1, 100)\n",
    "\n",
    "preds = logistic_predict(iris_X_test, betas)\n",
    "\n",
    "converted=[]\n",
    "\n",
    "for pred in preds:\n",
    "    if pred>= 0.5:\n",
    "        converted+=[1]\n",
    "    else:\n",
    "        converted+=[0]\n",
    "    \n",
    "mae= np.mean(abs(converted-iris_y_test))\n",
    "mae\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
